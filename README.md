# mpdw_project


71802 AndrÃ© Santos
71921 Tiago Santos
70663 Maximo Volynets


## âœ… Phase 1 Progress Summary

### 2.1 Video Data and Metadata
- âœ… Parsed **ActivityNet Captions** dataset (`train.json`)
- âœ… Selected 10 videos with the most moments
- âœ… Extracted metadata:
  - `video_id`
  - `timestamps`
  - `captions`
  - `duration`
  - `YouTube URL`

---

### 2.2 Text-Based Search
- âœ… Set up **OpenSearch** and created custom index mappings
- âœ… Defined `caption_bow` field with BM25
- âœ… Indexed all selected video moments (189 total)
- âœ… Tested search with `match` queries (BM25)
- âœ… Verified results from text-based search

---

### 2.3 Embeddings Neighborhood
- âœ… Added `caption_vec` field as `knn_vector` (384-dim)
- âœ… Computed embeddings using `all-MiniLM-L6-v2` (Sentence-BERT)
- âœ… Indexed embeddings in OpenSearch
- âœ… Performed semantic search using KNN vector queries
- âœ… Compared BM25 vs. semantic search results
- âŒ *Embeddings not persisted to file (optional step skipped)*

---

### 2.4 Constrained Embedding Search â€“ **Next Steps**
- âœ… Text based search, e.g., for keyword-based search;
- âœ…Embeddings based search; e.g., for semantic search;
- âœ… Add support for **filterable fields** (e.g. duration, tags)
- âœ… Implement search with **boolean filters** (e.g., `"woman" AND duration < 180s`)
- âœ… Propose and evaluate optimal index mappings  

---

### 2.5 Contextual Embeddings & Self-Attention â€“ **Pending**
- âœ… Visualize contextual word embeddings (layer-wise)
- âœ… Analyze positional embeddings (repetition behavior)
- âœ… Examine self-attention in dual vs. cross encoders
- âœ… Visualize token attention across layers

---

### 2.6 Persistent Storage
- â¬› *Optional* â€“ not implemented
- ğŸ’¡ Considered unnecessary for the current small dataset

---

**âœ”ï¸ Phase 1 Completion: ~100%**
- All core indexing and search functionality is complete.
- Filtering and embedding visualization to be finalized in upcoming steps.



## â¬œ Phase 2 Progress Summary

### ğŸ”¹ 1. Cross-Modal Retrieval with CLIP

- â¬œ **Extract Keyframes**
  - Sample one frame every 2 seconds.
  - Save as JPEGs in `data/frames/`.

- â¬œ **Compute CLIP Embeddings**
  - Use CLIP to compute:
    - `image_vec` for keyframes.
    - `text_vec` for captions.

- â¬œ **Extend OpenSearch Index**
  - Add a new field:
    ```json
    "image_vec": { "type": "knn_vector", "dimension": 512 }
    ```

- â¬œ **Index Keyframes**
  - Index each frame with:
    - `video_id`, `timestamp`, `image_path`, `image_vec`.

- â¬œ **Implement Search Queries**
  - â¬œ Text â†’ Image
  - â¬œ Image â†’ Image
  - â¬œ (Optional) Text + Image â†’ Image

- â¬œ **Evaluate Retrieval**
  - Compare cross-modal vs. unimodal.
  - Log top-5 results for each query type.

---

### ğŸ”¹ 2. Visual Question Answering with LLaVA

- â¬œ **Set Up LLaVA**
  - Use the API or run locally (GPU â‰¥ 12 GB).

- â¬œ **Retrieval-Augmented VQA**
  - â¬œ Encode the visual question (text).
  - â¬œ Use CLIP to retrieve top-1 frame.
  - â¬œ Pass frame + question to LLaVA.
  - â¬œ Collect and log the answer.

- â¬œ **Evaluate VQA**
  - Prepare 10â€“20 questions per video.
  - Manual or automatic assessment.

---

### ğŸ”¹ 3. Interpretability of LVLMs

- â¬œ **Attention Maps**
  - Visualize attention weights from CLIP and/or LLaVA.

- â¬œ **Relevancy Maps**
  - Apply Grad-CAM or similar over image inputs.

- â¬œ **Causal Graphs (Advanced)**
  - Explore masking-based influence on outputs.

- â¬œ **Analysis**
  - Discuss differences in focus between questions, images, and answers.
  - Identify hallucination or bias cases.

---

### ğŸ’¾ Optional: Persistent Storage

- â¬œ Use `pickle`, `HDF5`, or `parquet` to store:
  - CLIP embeddings
  - VQA answers

---

### ğŸ“ Reporting Guidelines

Include this phase as a 5-page section in your report:
- CLIP + cross-modal indexing
- Llava + QA pipeline
- Retrieval/VQA results
- Interpretability insights

Report latex link: https://www.overleaf.com/2958122766xhmchvsvpwyj#6c7348