{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f443f5c8-4aeb-46c9-a24c-78ee8efb9e66",
   "metadata": {},
   "source": [
    "# Phase 2: Large Vision and Language Models\n",
    " This notebook implements cross-modal retrieval and visual question answering using CLIP and Llava."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ddfa83-08ca-400c-bf3f-8d1e039cf484",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b999fe46-2e59-45b2-9394-6ba31ca57610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from sentence_transformers import util\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6337fada-5d44-45f7-92bb-5aac912d9230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910ebfe2-bec7-41d5-9aaa-e05551fe373c",
   "metadata": {},
   "source": [
    "## Load video metadata and captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our 10 selected video IDs from phase 1\n",
    "selected_ids = [\n",
    "    \"v_-rKS00dzFxQ\", # How to Cook Mashed Potatoes\n",
    "    \"v_-fjUWhSM6Hc\", # London 2012 Olympics\n",
    "    \"v_v7o9uSu9AVI\", # 20 Exercises on Parallel Bars\n",
    "    \"v_RJpWgi0EaUE\", # Vin Diesel Breakdancing Throwback\n",
    "    \"v_G7kqlq8WhRo\", # Twickenham Festival 2015 Tug of War\n",
    "    \"v_jTMdMnbW9OI\", # Washing my Face\n",
    "    \"v_9wtMJoqGTg0\", # Girl in Balance Beam (gymnastics)\n",
    "    \"v_Ffi7vDa3C2I\", # Epic Rollerblading Montage 80s\n",
    "    \"v_JRr3BruqS2Y\", # 'What U think about Rollerblading?'\n",
    "    \"v_Mkljhl3D9-Q\", # Preparing Angel Hair Pasta\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4661cd44-708e-48e7-b3f0-f82742bf676d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'duration': 231.43,\n",
       " 'timestamps': [[8.1, 23.14],\n",
       "  [24.3, 39.34],\n",
       "  [40.5, 53.23],\n",
       "  [54.39, 60.17],\n",
       "  [61.33, 62.49],\n",
       "  [63.64, 72.9],\n",
       "  [78.69, 82.16],\n",
       "  [83.32, 84.47],\n",
       "  [85.63, 91.42],\n",
       "  [92.57, 102.99],\n",
       "  [104.15, 105.3],\n",
       "  [114.56, 123.82],\n",
       "  [124.97, 131.92],\n",
       "  [133.07, 136.55],\n",
       "  [137.7, 152.75],\n",
       "  [153.9, 160.85],\n",
       "  [162, 167.79],\n",
       "  [168.95, 177.05],\n",
       "  [178.2, 180.52],\n",
       "  [181.68, 196.72],\n",
       "  [197.88, 210.6],\n",
       "  [211.76, 217.55]],\n",
       " 'sentences': ['A man and a woman stand by a table speaking to the camera.',\n",
       "  ' A recipe of mashed potatoes sits on the table.',\n",
       "  ' The man peels and cuts potatoes before throwing them into a yellow pot.',\n",
       "  ' The man throws in three handfuls of salt into the yellow pot.',\n",
       "  ' The man cuts up some garlic.',\n",
       "  ' Into the pot, the man throws the garlic along with two leaves.',\n",
       "  ' The woman joins the man at the table and she cuts butter in half.',\n",
       "  ' The woman throws the butter into a saucepan.',\n",
       "  ' The man speaks to her briefly and the woman proceeds to pour some milk into the saucepan.',\n",
       "  ' The man places the yellow pot and the saucepan on the stove.',\n",
       "  \" The man grabs the pot's lid and covers the pot.\",\n",
       "  ' Back at the table, the man removes the lid from the pot and removes the leaves.',\n",
       "  ' The man mashes the potatoes.',\n",
       "  ' The man throws in a pinch of salt onto the potatoes.',\n",
       "  ' The man resumes mashing the potatoes.',\n",
       "  ' The man pours in the milk from the saucepan into the pot.',\n",
       "  ' The man mashes the potatoes some more.',\n",
       "  ' The man grabs some pepper and throws it onto the potatoes.',\n",
       "  ' The man adds some more milk.',\n",
       "  ' The man returns to mashing the potatoes and stirring them as they have thoroughly become soft.',\n",
       "  ' The man pauses for a second to throw in some water and resumes to stirring the potatoes.',\n",
       "  ' The finished mashed potatoes dish is left on a counter.']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_activitynet_subset(json_path, selected_ids):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    videos = {}\n",
    "    for video_id in selected_ids:\n",
    "        entry = data.get(video_id)\n",
    "        if not entry:\n",
    "            continue\n",
    "        videos[video_id] = {\n",
    "            \"duration\": entry[\"duration\"],\n",
    "            \"timestamps\": entry[\"timestamps\"],\n",
    "            \"sentences\": entry[\"sentences\"],\n",
    "        }\n",
    "    return videos\n",
    "\n",
    "\n",
    "videos = load_activitynet_subset(\"train.json\", selected_ids)\n",
    "videos.get('v_-rKS00dzFxQ')   # first id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2011af-4ed1-4dc8-98f3-11d111b771b0",
   "metadata": {},
   "source": [
    "## Extract keyframes from videos\n",
    "(1 every 2 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0718dcfb-6406-43dc-bd35-70bd60efeb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keyframes(video_path, output_folder, frame_interval=2):\n",
    "    # TO DO: Use library OpenCV to extract frames every `frame_interval` seconds\n",
    "    # Save frames as JPEG in `output_folder/video_id/frame_0001.jpg`\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a9fe33-9dd2-4314-9a81-5038844b5c0d",
   "metadata": {},
   "source": [
    "## Compute CLIP embeddings - frames and captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f247ee49-05a8-4a71-9b3e-7a9e926d0be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9293a71a1b9f4e6bab4e6ba22197f1af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tneto\\anaconda3\\envs\\nlp-cv-ir\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\tneto\\.cache\\huggingface\\hub\\models--openai--clip-vit-base-patch32. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "599a2f2475c54f5ebbbc6ec121f0f8ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e50983a71dbf49f2a571def99bef6aff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b4b4353afa445118a293293c5f1da2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f91e76ff9bd740e9bc11086535c1f8e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90c9f0b5b4a24863a270c74897f0e38e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8cadccc379447a99ac4e5d099e408b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1398e9257a7f4da983517b8b7e84cc47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "945a04903a2647adbacaaa9275fd4104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2fc5cae-7ef2-4420-8247-5ad1431683ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_clip_image_embedding(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        embedding = clip_model.get_image_features(**inputs)\n",
    "    return embedding.squeeze().cpu().numpy()\n",
    "\n",
    "def compute_clip_text_embedding(text):\n",
    "    inputs = clip_processor(text=[text], return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        embedding = clip_model.get_text_features(**inputs)\n",
    "    return embedding.squeeze().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test embbedings ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45816333-0990-4222-baea-c63e81352189",
   "metadata": {},
   "source": [
    "## Index keyframes and embeddings in OpenSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae61d17-b40e-43ac-aef6-57ab66e1986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_clip_embeddings_to_opensearch(index_name, frames_folder, metadata_dict):\n",
    "    # TO DO: For each frame, compute image embedding and store in OpenSearch using appropriate mappings\n",
    "    # Example of fields: video_id, frame_time, image_vec, caption, caption_vec\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d7023a-bdfe-41f0-9ea2-fc215db16b2d",
   "metadata": {},
   "source": [
    "## Cross-Modal Retrieval\n",
    "text -> image  \n",
    "image -> image  \n",
    "image -> text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee266aa-325b-40bb-887d-1345b0952716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_image_by_text(query_text, index_name):\n",
    "    # TO DO: Use CLIP text embedding and knn_vector query on OpenSearch over `image_vec`\n",
    "    return 0\n",
    "\n",
    "def search_image_by_image(query_image_path, index_name):\n",
    "    # TO DO: Use CLIP image embedding and knn_vector query on OpenSearch over `image_vec`\n",
    "    return 0\n",
    "\n",
    "def search_text_by_image(query_image_path, index_name):\n",
    "    # TO DO: Use CLIP image embedding and knn_vector query on OpenSearch over `image_vec` (?)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258d4eea-4a48-4fe9-a197-d7c193f4650d",
   "metadata": {},
   "source": [
    "## Visual question answer with Llava"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a64edb8-5343-4329-80c7-a0739dd9d0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question_with_llava(image_path, question):\n",
    "    # TO DO: Use Llava API or local server to send image + question and receive a text answer\n",
    "    # Example of payload: { \"image\": <image bytes>, \"question\": \"What is the man doing?\" }\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f87e8b-34d6-45c4-b8ff-90068c99ff9c",
   "metadata": {},
   "source": [
    "## LVLM Interpretability\n",
    "Attention Maps,\n",
    "Relevancy Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e324d557-921b-403a-8321-d5823f24ae61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Use attention weights or Grad-CAM visualization to highlight image/text importance\n",
    "# instrument the CLIP or LLaVA model to extract internal attention values\n",
    "\n",
    "# ???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c02f236-6fba-4775-905e-ba95b7c5a850",
   "metadata": {},
   "source": [
    "## Save/load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba2e98c6-b5dc-4076-8c81-93bcef17d01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings_to_file(data_dict, output_path):\n",
    "    import pickle\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        pickle.dump(data_dict, f)\n",
    "\n",
    "def load_embeddings_from_file(path):\n",
    "    import pickle\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embeddings_to_file(embeddings, \"clip_image_embeddings.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-cv-ir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
