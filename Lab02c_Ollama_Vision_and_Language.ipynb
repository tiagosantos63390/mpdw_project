{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eb93513-963a-491a-a3ec-67897e160904",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollama in c:\\users\\tneto\\anaconda3\\envs\\nlp-cv-ir\\lib\\site-packages (0.4.8)\n",
      "Requirement already satisfied: httpx<0.29,>=0.27 in c:\\users\\tneto\\anaconda3\\envs\\nlp-cv-ir\\lib\\site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in c:\\users\\tneto\\anaconda3\\envs\\nlp-cv-ir\\lib\\site-packages (from ollama) (2.10.3)\n",
      "Requirement already satisfied: anyio in c:\\users\\tneto\\anaconda3\\envs\\nlp-cv-ir\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (4.6.2)\n",
      "Requirement already satisfied: certifi in c:\\users\\tneto\\anaconda3\\envs\\nlp-cv-ir\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\tneto\\anaconda3\\envs\\nlp-cv-ir\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\tneto\\anaconda3\\envs\\nlp-cv-ir\\lib\\site-packages (from httpx<0.29,>=0.27->ollama) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\tneto\\anaconda3\\envs\\nlp-cv-ir\\lib\\site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\tneto\\anaconda3\\envs\\nlp-cv-ir\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\tneto\\anaconda3\\envs\\nlp-cv-ir\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\tneto\\anaconda3\\envs\\nlp-cv-ir\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (4.12.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\tneto\\anaconda3\\envs\\nlp-cv-ir\\lib\\site-packages (from anyio->httpx<0.29,>=0.27->ollama) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\tneto\\anaconda3\\envs\\nlp-cv-ir\\lib\\site-packages (from anyio->httpx<0.29,>=0.27->ollama) (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4235d875-0fe1-4cda-aafb-8826f30e4041",
   "metadata": {},
   "source": [
    "# Ollama Vision and Language API\n",
    "\n",
    "Ollama is an LLM/LVLM model server that optimizes resources and allows to run multiple models on the same GPU.\n",
    "This notebook provides you with a few use cases with a Llava model. You can provide the model with an instruction and an image, and ask it to generate the answer. Note that Llava supports both language-only requests and language-and-vision requests.\n",
    "\n",
    "The complete documentation is available here:\n",
    " - https://github.com/ollama/ollama-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f62019e-2c65-47b4-94d6-4db42055d288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "\n",
    "client = Client(\n",
    "  host='https://twiz.novasearch.org/ollama',\n",
    "  headers={'x-some-header': 'some-value'}\n",
    ")\n",
    "\n",
    "model_multimodal = 'llava-phi3:latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ba4710-b603-424c-800d-a450fbd30a52",
   "metadata": {},
   "source": [
    "### Example 1: Generative answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06eb0a90-3509-4245-b2c0-e17d28416294",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.generate(model=model_multimodal, prompt='Why is the sky blue?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "917f9f0a-d16f-46bc-baa3-35194306a519",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky appears blue because of a phenomenon called Rayleigh scattering. This occurs when sunlight, which consists of different colors with varying wavelengths, interacts with the Earth's atmosphere. The molecules in the atmosphere are more effective at scattering shorter-wavelength light (such as blue and violet) than longer-wavelength light (such as red and orange). As a result, when sunlight passes through the atmosphere, it gets scattered in all directions, with the blue and violet light being scattered the most. However, our eyes are more sensitive to blue light, which is why we perceive the sky as blue rather than violet.\n"
     ]
    }
   ],
   "source": [
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d928c61c-545d-40f3-a1f4-1fad5df2e9c9",
   "metadata": {},
   "source": [
    "### Example 2: Chat template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7fd8aa0-1d33-40cc-b29e-f4753ac2ccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat(model=model_multimodal, messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "  },\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the grass green?',\n",
    "  }\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca40e585-a4b1-4cc9-b13c-1731f6e822eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky appears blue because of a phenomenon called Rayleigh scattering. When sunlight passes through Earth's atmosphere, it encounters molecules and small particles that are much smaller than its wavelength. These molecules and particles scatter the light in different directions. Blue light has a shorter wavelength than other colors, so it is scattered more than red or yellow light. This means that when we look up at the sky, we see mostly blue light rather than any other color.\n",
      "\n",
      "The grass appears green because of the pigment chlorophyll, which is found in plant cells. Chlorophyll absorbs most wavelengths of visible light except for green, which it reflects back to our eyes. This gives plants their characteristic green color.\n"
     ]
    }
   ],
   "source": [
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5633ddc-404b-4a18-9dc3-1107aa24b40b",
   "metadata": {},
   "source": [
    "### Example 3: Image captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c77fecb-492b-4142-afd5-36bda06f75d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img = 'data/frames/v_-rKS00dzFxQ/frame_0041.jpg'\n",
    "image = Image.open(img)\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96fae565-cfc7-4589-bb86-f01de0d42d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat(model=model_multimodal, messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Describe this image.',\n",
    "    'images': [img]\n",
    "  },\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02e8a60c-2454-426e-8c71-3e1312763c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the heart of a cozy kitchen, two people are engrossed in an activity that speaks volumes about their shared interest - cooking. The person on the left, donned in a brown apron, is holding a white spoon with a piece of food on it, perhaps tasting or inspecting it. On the right, another individual stands by attentively, also dressed in an apron and holding a wooden spoon.\n",
      "\n",
      "The kitchen counter they stand behind is laden with various items - two pots hint at simmering concoctions, a bowl possibly filled with ingredients yet to be added, and a yellow dish that adds a pop of color to the scene. The backdrop features a brick wall that exudes an old-world charm and a window offering a glimpse into the world outside.\n",
      "\n",
      "The image is rich in detail and activity, painting a picture of two people engaged in the joyous art of cooking together.\n"
     ]
    }
   ],
   "source": [
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c683ac-acb5-49e5-a8fe-cb1e6a3d8417",
   "metadata": {},
   "source": [
    "### Example 4: Visual question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6eae17e4-9f14-4345-af62-4bcb43b19645",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat(model=model_multimodal, messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Is the dog running or jumping?.',\n",
    "    'images': ['data/frames/v_-rKS00dzFxQ/frame_0041.jpg']\n",
    "  },\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83e923a7-9867-42e0-83ce-bf8ee9593a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The woman is holding a potato in her hands, and the man is reaching for something. There are no dogs present in the image. It appears that they might be at an event with food, including bowls of cheese dip, spoons, and cups. Additionally, there is a table with other items such as plates, bowls, vases, and flowers on it.\n"
     ]
    }
   ],
   "source": [
    "print(response.message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-cv-ir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
